{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "import numpy as np      # pip install numpy\n",
    "import time\n",
    "from mnist import MNIST # pip install python-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tue Jul  7 20:15:54 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.138                Driver Version: 390.138                   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Quadro K1100M       Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   56C    P0    N/A /  N/A |    671MiB /  1999MiB |     64%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1122      G   /usr/lib/xorg/Xorg                            82MiB |\n|    0      1717      G   /usr/lib/xorg/Xorg                           240MiB |\n|    0      1957      G   /usr/bin/gnome-shell                         158MiB |\n|    0      2745      G   ...uest-channel-token=17668071954511488240   176MiB |\n+-----------------------------------------------------------------------------+\n"
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "np.random.seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "test_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def relu(x):\n",
    "    return x*(x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU derivative\n",
    "def d_relu(a):\n",
    "    return a>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x, axis=0, keepdims=True))\n",
    "    return e_x/np.sum(e_x, axis=0, keepdims=True)\n",
    "# Softmax2\n",
    "def softmax2(x):\n",
    "    e_x = np.exp(x-np.max(x, axis=1, keepdims=True))\n",
    "    return e_x/np.sum(e_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def activation_switcher(activation, x):\n",
    "    switcher = {\n",
    "        'relu': relu(x),\n",
    "        'softmax': softmax(x),\n",
    "        'softmax2': softmax2(x)\n",
    "    }\n",
    "    return switcher.get(activation, \"Invalid activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    # Constructor\n",
    "    def __init__(self, layers, activations):\n",
    "        self.activations = activations\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        for i in range(len(layers)-1):\n",
    "            # random_rate = 0\n",
    "            # random_rate = 0.1\n",
    "            random_rate = np.sqrt(2/layers[i])\n",
    "            # random_rate = np.sqrt(1/layers[i])\n",
    "            # random_rate = np.sqrt(2/(layers[i+1] + layers[i]))\n",
    "            w = np.random.randn(layers[i+1], layers[i])*random_rate\n",
    "            b = np.random.randn(layers[i+1],1)*random_rate\n",
    "            self.weights.append(w)\n",
    "            self.bias.append(b)\n",
    "    # Feed forward with 1 data point (2D-array)\n",
    "    def feed_forward(self, x):\n",
    "        a = [x]\n",
    "        for w, b, act_func in zip(self.weights, self.bias, self.activations):\n",
    "            temp_a = np.matmul(w, a[-1]) + b\n",
    "            temp_a = activation_switcher(act_func, temp_a)\n",
    "            a.append(temp_a)\n",
    "        return a\n",
    "    # Feed forward with x_train (3D-array)\n",
    "    def forward_loss(self,x):\n",
    "        a = [x]\n",
    "        for w, b, act_func in zip(self.weights, self.bias, self.activations):\n",
    "            if act_func == 'softmax':\n",
    "                act_func = 'softmax2'\n",
    "            temp_a = np.matmul(w, a[-1]) + b\n",
    "            temp_a = activation_switcher(act_func, temp_a)\n",
    "            a.append(temp_a)\n",
    "        return a\n",
    "    # Backpropagation\n",
    "    def back_propagation(self, a, y):\n",
    "        # a = self.feed_forward(x)\n",
    "        size = len(self.weights) + 1\n",
    "        delta = [0]*size\n",
    "        delta[-1] = a[-1]-y\n",
    "        cur = size-2\n",
    "        while cur>=1:\n",
    "            delta[cur] = np.matmul(self.weights[cur].transpose(), delta[cur+1])*d_relu(a[cur])\n",
    "            cur -= 1\n",
    "        return delta \n",
    "    # Update weights and biases\n",
    "    def update_weights(self, l_rate, a, delta):\n",
    "        size = len(self.weights)\n",
    "        for i in range(size):\n",
    "            self.weights[i] -= l_rate*np.matmul(delta[i+1], a[i].transpose())\n",
    "            self.bias[i] -= l_rate*delta[i+1]\n",
    "    # Calculate mean loss value with all data (3D-array)\n",
    "    def calculate_loss(self, x, y):\n",
    "        a = self.forward_loss(x)\n",
    "        error = -1.0*y*np.log(a[-1])\n",
    "        error = 1/x.shape[0]*np.sum(error)\n",
    "        return error\n",
    "    # Train model\n",
    "    def train_model(self, x_train, y_train, n_epochs=5, l_rate=0.001):\n",
    "        print(\"Start training with {} epochs and learning rate = {}\".format(n_epochs, l_rate))\n",
    "        for i in range(n_epochs):\n",
    "            beg = time.time()\n",
    "            print(\"Epoch {:>2d} - [\".format(i+1),end='')\n",
    "            for j in range(x_train.shape[0]):\n",
    "                a = self.feed_forward(x_train[j])\n",
    "                delta = self.back_propagation(a, y_train[j])\n",
    "                self.update_weights(l_rate, a, delta)\n",
    "                if (j+1)%6000==0:\n",
    "                    print(\"=\",end='')\n",
    "            train_err = self.calculate_loss(x_train, y_train)\n",
    "            test_err = self.calculate_loss(x_test, y_test)\n",
    "            end = time.time()\n",
    "            train_loss_history.append(train_err)\n",
    "            test_loss_history.append(test_err)\n",
    "            print(\"] - {:>.2f} (s) - train_loss: {:>.6f} - test_loss: {:>.6f}\".format(end-beg, train_err, test_err))\n",
    "    # Predict on test dataset (3D-array)\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        a = self.forward_loss(x_test)\n",
    "        y_hat = a[-1]\n",
    "        predict = np.argmax(y_hat, axis=1)\n",
    "        actual = np.argmax(y_test, axis=1)\n",
    "        result = predict == actual\n",
    "        correct = result[result==True]\n",
    "        accuracy = len(correct)/len(result)\n",
    "        # print(\"Accuracy = {:>.2f}%\".format(accuracy*100))\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, x):\n",
    "        a = self.feed_forward(x)\n",
    "        y_hat = a[-1]\n",
    "        predict = np.argmax(y_hat)\n",
    "        print(\"Predict = {:>1d}\".format(predict))\n",
    "        return y_hat[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading data...Done! Time = 1.90 (s)\n"
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\", end='')\n",
    "be = time.time()\n",
    "mnist = MNIST('./samples/')\n",
    "x_train, labels_train = mnist.load_training()\n",
    "x_test, labels_test = mnist.load_testing()\n",
    "en = time.time()\n",
    "print(\"Done! Time = {:>.2f} (s)\".format(en-be))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data normalization...Done! Time = 3.44 (s)\n"
    }
   ],
   "source": [
    "# Data normalization\n",
    "print(\"Data normalization...\", end='')\n",
    "be = time.time()\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],784,1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],784,1))\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "en = time.time()\n",
    "print(\"Done! Time = {:>.2f} (s)\".format(en-be))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Encoding target...Done! Time = 0.10 (s)\n"
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "print(\"Encoding target...\", end='')\n",
    "be = time.time()\n",
    "y_train = []\n",
    "y_test = []\n",
    "for label in labels_train:\n",
    "    arr = np.zeros((10,1))\n",
    "    arr[label] = 1\n",
    "    y_train.append(arr)\n",
    "for label in labels_test:\n",
    "    arr = np.zeros((10,1))\n",
    "    arr[label] = 1\n",
    "    y_test.append(arr)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "en = time.time()\n",
    "print(\"Done! Time = {:>.2f} (s)\".format(en-be))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Start training with 10 epochs and learning rate = 0.005\nEpoch  1 - [==========] - 68.83 (s) - train_loss: 0.119281 - test_loss: 0.128666\nEpoch  2 - [==========] - 65.37 (s) - train_loss: 0.071841 - test_loss: 0.094893\nEpoch  3 - [==========] - 66.55 (s) - train_loss: 0.054026 - test_loss: 0.086796\nEpoch  4 - [==========] - 67.10 (s) - train_loss: 0.041104 - test_loss: 0.086167\nEpoch  5 - [==========] - 66.47 (s) - train_loss: 0.032822 - test_loss: 0.083128\nEpoch  6 - [==========] - 67.06 (s) - train_loss: 0.031612 - test_loss: 0.093133\nEpoch  7 - [==========] - 85.40 (s) - train_loss: 0.021743 - test_loss: 0.074750\nEpoch  8 - [==========] - 90.05 (s) - train_loss: 0.019312 - test_loss: 0.083941\nEpoch  9 - [==========] - 83.78 (s) - train_loss: 0.020827 - test_loss: 0.088799\nEpoch 10 - [==========] - 66.92 (s) - train_loss: 0.014623 - test_loss: 0.075508\n"
    }
   ],
   "source": [
    "layers = [784, 256, 128, 10]\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "n_epochs = 10\n",
    "nn = NeuralNetwork(layers, activations)\n",
    "# Train \n",
    "nn.train_model(x_train, y_train, n_epochs=n_epochs, l_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on test set = 98.01%\n"
    }
   ],
   "source": [
    "# Evaluate\n",
    "accuracy = nn.evaluate(x_test, y_test)*100\n",
    "print(\"Accuracy on test set = {:>.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session(\"model.db\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}